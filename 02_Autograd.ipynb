{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_Autograd\n",
    "In this notebook, we will see how to compute the derative of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T06:28:33.267403Z",
     "start_time": "2017-08-07T06:28:32.871407Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Graph & Gradient Computation\n",
    "\n",
    "It is the `autograd` package that provides automatic differentiation for all operations on Tensors.\n",
    "It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "If you set the attribute `.requires_grad` of some tensor as `True`, it starts to track all operations on it.\n",
    "When you finish your computation you can call `.backward()` and have all the gradients computed automatically.\n",
    "The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a `Function`.\n",
    "\n",
    "`Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a `Function` that has created the `Tensor` (except for Tensors created by the user - their `grad_fn` is None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1)  # create a tensor with requires_grad=False (default)\n",
    "print(x.requires_grad)\n",
    "\n",
    "y = torch.ones(1)  # another tensor with requires_grad=False\n",
    "z = x + y\n",
    "\n",
    "print(z.requires_grad)\n",
    "\n",
    "# then autograd won't track this computation. let's verify!\n",
    "# z.backward()\n",
    "\n",
    "w = torch.ones(1, requires_grad=True)\n",
    "print(w.requires_grad)\n",
    "\n",
    "# add to the previous result that has require_grad=False\n",
    "total = w + z\n",
    "# the total sum now requires grad!\n",
    "print(total.requires_grad)\n",
    "\n",
    "# no computation is wasted to compute gradients for x, y and z, which don't require grad\n",
    "print(z.grad == x.grad == y.grad == None)\n",
    "\n",
    "# you can also manually enable gradients for a tensor, but use this with caution!\n",
    "x = torch.ones(1)\n",
    "print(x.requires_grad)\n",
    "x.requires_grad_(True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T06:28:36.056019Z",
     "start_time": "2017-08-07T06:28:36.044471Z"
    }
   },
   "outputs": [],
   "source": [
    "# create graph\n",
    "\n",
    "x = torch.tensor([3], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "y = 2*x +3\n",
    "\n",
    "print(x, y)\n",
    "print(x.requires_grad, y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T06:28:36.478303Z",
     "start_time": "2017-08-07T06:28:36.471335Z"
    }
   },
   "outputs": [],
   "source": [
    "print(y.grad_fn, y.grad_fn.next_functions[0][0], y.grad_fn.next_functions[0][0].next_functions[0][0], sep='\\n')\n",
    "print(y.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T06:28:36.920118Z",
     "start_time": "2017-08-07T06:28:36.886343Z"
    }
   },
   "outputs": [],
   "source": [
    "y.backward()  # calculate dy / dx  == d(2*x + 3) / dx  == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-07T06:28:37.262640Z",
     "start_time": "2017-08-07T06:28:37.255347Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x, x.grad) # dy / dx is stored\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.detach()\n",
    "print(z, z.requires_grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent tracking history (and using memory), you can also wrap the code block in `with torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don’t need the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "print(y.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
